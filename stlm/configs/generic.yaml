# stlm/configs/sft.yaml


model:
  tokenizer:
    # type: "huggingface"
    # model_name: "Qwen/Qwen3-0.6B"
    type: "bytebpe"
    vocab_size: 5257

  embedder:
    # name: "huggingface"
    # model_name: "Qwen/Qwen3-0.6B"
    name: "standard"
    max_position_embeddings: 1024
    hidden_size: 512
    dropout: 0.1
    stride: 1024
  core:
    # name: "huggingface"
    # model_name: "Qwen/Qwen3-0.6B"
    name: "standard"
    num_layers: 12
    attention:
      name: "standard"
      num_heads: 16
      dropout: 0.1
    ffn:
      name: "standard"
      intermediate_size: 2048
      activation: "gelu"
      dropout: 0.1
  head:
    # name: "huggingface"
    # model_name: "Qwen/Qwen3-0.6B"
    name: "standard"
    tie_weights: true
    

# lora:
#   target_modules: ["q_proj", "v_proj"] #, "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj" ]
#   rank: 8
#   alpha: 16
#   dropout: 0.05
#   exclude_modules: []

trainer:
  max_iterations: 10000
  lr: 1e-4
  grad_accum_steps: 4
  epochs: 1
  batch_size: 64
  save_interval: 1000
  eval_interval: 500
  checkpointing: true
  dataset:
    path: "wikimedia/wikipedia"
    name: "20231101.simple"
    splits: ["train"]
    text_column: "text"
    val_split: 0.01
  scheduler:
    name: "cosineannealinglr"
    warmup_ratio:   0.1
    decay_ratio:    0.8
    params:
      eta_min: 1e-6

general:
  load_checkpoint: None
  paths:
    data_dir: ${hydra:runtime.cwd}/data          # always points to your project root data/
    outputs_dir: ${hydra:runtime.output_dir}     # dynamically resolves to this runâ€™s output folder
  wandb:
    wandb_log: true
    project_name: supertinylanguagemodels