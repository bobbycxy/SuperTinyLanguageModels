# stlm/configs/sft.yaml


model:
  tokenizer:
    # type: "huggingface"
    # model_name: "Qwen/Qwen3-0.6B"
    type: "bytebpe"
    vocab_size: 5000
    save_path: "byte_bpe.json"

  embedder:
    # name: "huggingface"
    # model_name: "Qwen/Qwen3-0.6B"
    name: "standard"
    max_position_embeddings: 1024
    hidden_size: 512
    dropout: 0.1
  core:
    # name: "huggingface"
    # model_name: "Qwen/Qwen3-0.6B"
    name: "standard"
    num_layers: 12
    attention:
      name: "standard"
      num_heads: 8
      dropout: 0.1
    ffn:
      name: "standard"
      intermediate_size: 2048
      activation: "gelu"
      dropout: 0.1
  head:
    # name: "huggingface"
    # model_name: "Qwen/Qwen3-0.6B"
    name: "standard"
    tie_weights: true
    

# lora:
#   target_modules: ["q_proj", "v_proj"] #, "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj" ]
#   rank: 8
#   alpha: 16
#   dropout: 0.05
#   exclude_modules: []

trainer:
  max_iterations: 10000
  lr: 1e-4
  grad_accum_steps: 4
  epochs: 1
  batch_size: 64
  save_interval: None
  eval_interval: 500
  checkpointing: true
  dataset:
    path: "wikimedia/wikipedia"
    name: "20231101.simple"
    splits: ["train"]
    text_column: "text"
    val_split: 0.01

general:
  paths:
    data_dir: "data"
  wandb:
    wandb_log: true
    project_name: supertinylanguagemodels
